# Competitive Intelligence Skill

AI-powered competitive intelligence reports that answer 4 strategic questions: **Who are we competing with? Why are they winning? Where's the whitespace? What should we do?**

Built for Ops & Data Analysts, Product Teams, and Strategy Teams. Replaces 1â€“2 days of manual research with ~30 min AI research + user review.

---

## Quick Start

### 1. Prepare input
Copy `skill/input-template.md` â†’ fill in your product details (name, description, features, narrative).

### 2. Run in Claude chat
New conversation â†’ paste the **Prompt Template** below â†’ attach your filled input.

### 3. Review & save
Download the `.md` report â†’ commit to `reports/`.

### 4. Iterate
Note feedback in `changelog.md`. Every 3â€“5 notes â†’ update `skill/spec.md` + `skill/SKILL.md`.

---

## Prompt Template

```
You are a competitive research expert. I will send you a product spec.
Run a competitive intelligence report following this workflow:

Step A: Parse input â†’ confirm product info back to me
Step B: I confirm â†’ you begin research
Step C: Competitor Discovery & Battlefield Mapping (web search)
Step D: Deep Dive each competitor â€” Positioning vs Execution, multi-source evidence
Step E: Strategic Synthesis â€” who's winning & why, whitespace, threats
Step F: Generate .md report with 8.5 sections

Hard rules:
- Source confidence tier [A]â€“[D] for every source
- Freshness gate: metrics prefer â‰¤3 months, fallback â‰¤12 months + flag âš ï¸. >12 months drop.
- Search queries for metrics MUST include date filter (year, "latest", "recent")
- Source priority ladder: use P1 source first (DefiLlama > Dune > media for on-chain)
- Deep dive selection: score 5 criteria (ICP 30, Feature 25, BizModel 20, Traction 15, Activity 10)
- Every claim must have source + "as of [date]"
- Clearly label Fact vs Inference
- Separate Positioning (what they SAY) vs Execution (what they DO)
- Strengths/weaknesses from external sources, not AI opinion
- Each deep dive covers â‰¥2 of 4 sources: Community, Expert, News, On-chain/Market
- â‰¥2 actionable whitespace opportunities with build tickets
- â‰¥1 threat rated High or Critical (unless justified low-threat environment)
- Action items specific enough to create tickets
- Self-assessment score: 5 dimensions Ã— 20 points
- Fallback proxy when metric unavailable (clearly label proxy)
- Output density: deep dive 400â€“600 words, whitespace 200â€“300 words

Auto-detect industry branch:
- ðŸ”— Crypto: TVL, volume, wallets, on-chain fees | DefiLlama, Dune, protocol dashboards
- ðŸ¢ Non-Crypto: MRR/ARR, pricing tiers, G2 rating, team size | G2, Capterra, SimilarWeb, Crunchbase

Here is my product spec:

[PASTE SPEC HERE]
```

---

## File Structure

```
competitive-intelligence/
â”œâ”€â”€ README.md                          â† You are here
â”œâ”€â”€ changelog.md                       â† Feedback log for iterations
â”œâ”€â”€ skill/
â”‚   â”œâ”€â”€ spec.md                        â† Source of truth (v3)
â”‚   â”œâ”€â”€ SKILL.md                       â† Claude instructions (v3)
â”‚   â””â”€â”€ input-template.md              â† Template for product brief
â””â”€â”€ reports/
    â”œâ”€â”€ pumpfun_Feb2026.md             â† v1 (archived)
    â”œâ”€â”€ pumpfun_v2_Feb2026.md          â† v2 (source tiers + selection rubric)
    â”œâ”€â”€ pumpfun_v3_Feb2026.md          â† v3 (freshness enforced) âœ… latest
    â””â”€â”€ Polymarket_Feb2026.md          â† v2 (pre-freshness)
```

---

## Report Structure (8.5 Sections)

| # | Section | Answers | Key Features |
|---|---------|---------|-------------|
| 1 | Battlefield Map | Who are we competing with? | ASCII diagram, 4 tiers (direct / indirect / emerging / substitutes), selection scores |
| 2 | Comparison Matrix | How do we compare? | ðŸŸ¢ðŸŸ¡ðŸ”´ ratings, standardized units, user product = column 1 |
| 3 | Deep Dive | What are they actually doing? | Positioning vs Execution layers, 4-source evidence, source tiers [A]â€“[D] |
| 4 | Who's Winning & Why | Why are they ahead or behind? | Per-factor analysis, each ends with "So what?" |
| 5 | Whitespace | Where can we attack? | Gap â†’ Evidence â†’ Actionable â†’ Why winnable â†’ Build ticket |
| 6 | Threats | What should we worry about? | Severity table with mitigation per threat |
| 7 | Action Items | What should we do? | Build / Watch / Benchmark tables with timelines |
| 8 | Sources & Confidence | How reliable is this? | URL + date + tier + age per source, confidence per section |
| 8.5 | Self-Assessment | How good is this report? | 5 dimensions Ã— 20 points, user override field |

---

## Core Philosophies

| # | Philosophy | One-liner |
|---|-----------|-----------|
| P1 | Decision-first | The report is a decision tool, not a data dump |
| P2 | Comparable > many data | Less data that's comparable > lots of scattered data |
| P3 | Evidence-first | No source â†’ "Unknown". Never fabricate. |
| P4 | Freshness matters | Metrics: prefer â‰¤3 months, fallback â‰¤12 months + flag. >12 months drop. |
| P5 | Positioning â‰  Execution | Separate what they SAY vs what they DO |
| P6 | Map the battlefield | Draw structure, don't just list |
| P7 | Find whitespace | Point to gaps you can attack |
| P8 | Actionable > academic | Every insight must answer "so what?" |

---

## Key Features (v3)

### Source Confidence Taxonomy
Every source labeled [A]â€“[D]. D-source claims flagged âš ï¸.

| Tier | Examples |
|------|---------|
| [A] | Official docs, on-chain indexers (DefiLlama, Dune), SEC filings, Wikipedia |
| [B] | Reputable media (The Block, Messari, CoinDesk), Sacra, Crunchbase, G2 |
| [C] | Community (Reddit, X threads, opinion blogs) |
| [D] | Unsourced aggregators, content farms |

### Freshness Enforcement (HR-19)

| Source Age | Metrics / Traction | Context / Background |
|-----------|-------------------|---------------------|
| â‰¤3 months | âœ… Preferred | âœ… OK |
| 3â€“12 months | âš ï¸ Fallback â€” use only if no â‰¤3 month source, flag "âš ï¸ Older" | âš ï¸ OK + flag |
| >12 months | âŒ Drop | âŒ Drop |

All metric search queries MUST include a date filter (`"2026"`, `"latest"`, `"recent"`).

### Deep Dive Selection Rubric
100-point scoring system determines which competitors get deep dived:

| Criteria | Weight |
|----------|--------|
| ICP Overlap | 30 |
| Feature Overlap | 25 |
| Business Model Overlap | 20 |
| Traction Relevance | 15 |
| Recent Activity | 10 |

Score all direct competitors â†’ rank â†’ deep dive top scorers â†’ show scores in Battlefield Map.

### Source Priority Ladder
Per metric type, always use the highest-priority source available:

| Metric Type | P1 | P2 | P3 | Fallback |
|------------|----|----|----|----|
| Traffic | SimilarWeb | Semrush | Ahrefs | "Unknown" |
| Funding | Official announcement | Crunchbase | Media | "Not publicly disclosed" |
| On-chain (crypto) | DefiLlama | Dune | Protocol docs | Media recap |
| Reviews (non-crypto) | G2 | Capterra | TrustRadius | "No review data" |

### Fallback Proxy Policy
When a primary metric is unavailable:

| Missing Metric | Proxy |
|---------------|-------|
| Traffic | App downloads / Google Trends branded search / on-chain wallets |
| Revenue | Funding stage as scale proxy / team size |
| Engagement rate | Follower count only + note "engagement unavailable" |

All proxies MUST be labeled: `"Proxy: [X] used because [Y] unavailable"`

### Industry Branch Detection
Auto-detected at Step B based on product description:

| Signal | Branch | Metrics Focus | Extra Sources |
|--------|--------|--------------|--------------|
| Token, chain, TVL, DeFi | ðŸ”— Crypto | TVL, volume, wallets, on-chain fees | DefiLlama, Dune, protocol dashboards |
| SaaS, pricing tiers, ARR | ðŸ¢ Non-Crypto | MRR/ARR, pricing, G2 rating, team size | G2, Capterra, SimilarWeb, Crunchbase |

### Output Density Guidelines

| Section | Target | Format |
|---------|--------|--------|
| Battlefield Map | 300â€“500 words + tables + diagram | â€” |
| Comparison Matrix | Table only | No prose between rows |
| Deep Dive (per competitor) | 400â€“600 words | Layer A ~100w, Layer B ~200w + table, Evidence ~100â€“200w, Threat ~50â€“100w |
| Who's Winning (per competitor) | 150â€“250 words | Lead with factor + evidence, end with "So what?" |
| Whitespace (per opportunity) | 200â€“300 words | Gap â†’ Evidence â†’ Actionable â†’ Why winnable â†’ Ticket |
| Threats | Table + 1â€“2 sentence mitigation | No narrative |
| Action Items | Bullet, â‰¤2 sentences per item | Tables for Build / Watch / Benchmark |
| Sources | Table only | URL + date + tier + age |
| Self-Assessment | Table + 1 sentence per dimension | No long justification |

### Self-Assessment Scoring

| Dimension | Max | Measures |
|-----------|-----|---------|
| Evidence Quality | 20 | Source count, tier distribution, coverage gaps |
| Comparability | 20 | Standardized units, fair comparison across competitors |
| Strategic Usefulness | 20 | Answers all 4 strategic questions clearly |
| Freshness | 20 | % sources â‰¤3 months, flags applied correctly |
| Actionability | 20 | Build tickets with timelines, specificity |

---

## Reports

| # | Product | Version | Date | Score | File |
|---|---------|---------|------|-------|------|
| 1 | pump.fun | v1 | Feb 2026 | â€” | [pumpfun_Feb2026.md](reports/pumpfun_Feb2026.md) |
| 2 | pump.fun | v2 | Feb 2026 | 81/100 | [pumpfun_v2_Feb2026.md](reports/pumpfun_v2_Feb2026.md) |
| 3 | pump.fun | v3 | Feb 2026 | 84/100 | [pumpfun_v3_Feb2026.md](reports/pumpfun_v3_Feb2026.md) |
| 4 | Polymarket | v2 | Feb 2026 | 87/100 | [Polymarket_Feb2026.md](reports/Polymarket_Feb2026.md) |

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| v1 | Feb 2026 | Initial 8-section report structure, 8 philosophies, pump.fun test |
| v2 | Feb 2026 | +Source tiers [A]â€“[D], +Selection rubric (100-pt scoring), +Priority ladder, +Fallback proxies, +Self-assessment scoring (5Ã—20), +Output density guidelines, +Crypto / Non-crypto branch detection, +Deep dive micro-example |
| v3 | Feb 2026 | +Freshness enforcement (HR-19): metrics prefer â‰¤3mo / fallback â‰¤12mo / >12mo drop. +Date filter required in search queries. +Age column in source table. +FM-11 stale source handling |

---

## Changelog

See [changelog.md](changelog.md) for detailed feedback notes per iteration.
